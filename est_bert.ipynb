{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kecCveCflvXJ"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/allenai/bi-att-flow/master/squad/evaluate-v1.1.py\n",
        "# !pip install sentencepiece\n",
        "# !pip install peft\n",
        "# !pip install sacrebleu\n",
        "# !git clone https://github.com/google-deepmind/xquad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import pipeline, AutoModelWithLMHead\n",
        "import torch\n",
        "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from transformers.pipelines import pipeline\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaCoiinNmBcc",
        "outputId": "918891bb-ebb4-4436-fa42-7917149cc77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch(iterable, n=1):\n",
        "    l = len(iterable)\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iterable[ndx:min(ndx + n, l)]"
      ],
      "metadata": {
        "id": "HX00WP_1mDX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_experiment(path_regex,pipe,test_fn, result_path, prefix, batch_size):\n",
        "    for file_path in glob.glob(path_regex):\n",
        "        file_name = os.path.split(file_path)[-1]\n",
        "        lang = file_name.split('.')[1]\n",
        "        results = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            xquad_json = json.load(f)\n",
        "            for data in xquad_json['data']:\n",
        "                for paragraph in data['paragraphs']:\n",
        "                  context = paragraph['context']\n",
        "                  questions = list(map(lambda x:x['question'], paragraph['qas']))\n",
        "                  ids = list(map(lambda x:x['id'], paragraph['qas']))\n",
        "                  for batch_question, batch_ids in zip(batch(questions, batch_size), batch(ids, batch_size)):\n",
        "                      model_answers = test_fn(context, batch_question, pipe)\n",
        "                      for model_answer, id in zip(model_answers, batch_ids):\n",
        "                          results.append({id:model_answer})\n",
        "\n",
        "        out_path = os.path.join(result_path, prefix + '/' + file_name)\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        with open(out_path, \"w+\", encoding='utf-8') as out_f:\n",
        "            json.dump(results, out_f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "HlqhgBCTmFBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"alon-albalak/bert-base-multilingual-xquad\"\n",
        "\n",
        "pipe = pipeline('question-answering', model=model_name, tokenizer=model_name, device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKgrSzZGmKtb",
        "outputId": "f9d215b0-658c-4028-abe8-d940d68755c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_function(context, questions, pipe):\n",
        "    input = [{'question': question,'context': context} for question in questions]\n",
        "    output = pipe(input, batch_size=len(input))\n",
        "    if isinstance(output, list):\n",
        "       return [x['answer'] for x in output]\n",
        "    return [output['answer']]"
      ],
      "metadata": {
        "id": "nz6BFmAhqMKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_regex = '/content/drive/MyDrive/xquad.??.json'\n",
        "do_experiment(path_regex,pipe,test_function, '/content/drive/MyDrive/', 'bert_csw', 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wzywn4RqUYs",
        "outputId": "bd69845e-259f-4189-c705-7579acf43e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}
