{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "mTnqMw9kCKwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwlMhSY1qgNx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-deepmind/xquad\n",
        "!apt-get install libgoogle-perftools-dev libsparsehash-dev\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install stanza\n",
        "!pip install langdetect\n",
        "!pip install sudachipy sudachidict_core\n",
        "!git clone https://github.com/qiyuw/WSPAlign.git\n",
        "!git clone https://github.com/qiyuw/WSPAlign.InferEval.git\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install numba==0.56.4\n",
        "!pip install sentence-splitter==1.4\n",
        "!pip install faiss-gpu==1.7.2\n",
        "!pip install googletrans==4.0.0rc1\n",
        "!git clone https://github.com/alihejazi97/bertalign.git\n",
        "!cd /content/bertalign/ && pip install .\n",
        "clear_output(wait=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoConfig,AutoModelForQuestionAnswering,AutoTokenizer,pipeline)\n",
        "import stanza\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from stanza.pipeline.core import DownloadMethod\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from bertalign import Bertalign\n",
        "from difflib import SequenceMatcher\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "ytPefsA-ZqAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU9h2T5sp5Fo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipe_align = pipeline(\"question-answering\", model=\"qiyuw/WSPAlign-mbert-base\", device=device)\n",
        "clear_output(wait=False)\n",
        "drive.mount('/content/drive')\n",
        "print(f'device = {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lang_codes = ['ar','de','el','en','es','fa','fr','hi','ja','ro','ru','sw','te','th','tr','vi','zh']\n",
        "lang_codes = ['hi', 'en']\n",
        "# 'bn' bengali is not supported\n",
        "for lang_code in lang_codes:\n",
        "    stanza.download(lang_code)\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "dibCklxKCwbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stanza_pipeline(lang_code, use_gpu=False):\n",
        "    return stanza.Pipeline(lang_code, use_gpu=use_gpu, download_method=None, tokenize_no_ssplit=True)\n",
        "\n",
        "# def contains_unmatched_snts_length(doc1, doc2, data_idx, context_idx):\n",
        "#     if len(doc1.sentences) != len(doc2.sentences):\n",
        "#         print(f'data_id = {data_idx} -- context_id = {context_idx}')\n",
        "#         print(f\"number of sentences don't match. src length = {len(doc1.sentences)} -- target length = {len(doc2.sentences)}\")\n",
        "#         return True\n",
        "#     return False\n",
        "\n",
        "def contains_illegal_phrases(phrase, illegal_phrases):\n",
        "    if not illegal_phrases:\n",
        "        return False\n",
        "    for illegal_phrase in illegal_phrases:\n",
        "        if phrase['src_text'] in illegal_phrase or illegal_phrase in phrase['src_text']:\n",
        "            return True\n",
        "        if SequenceMatcher(None, phrase['src_text'], illegal_phrase).find_longest_match().size > 4:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def align_sentences(src, tgt, src_lang, tgt_lang):\n",
        "    aligner = Bertalign(src, tgt, src_lang, tgt_lang)\n",
        "    aligner.align_sents()\n",
        "    src_lines = []\n",
        "    tgt_lines = []\n",
        "    for bead in (aligner.result):\n",
        "        src_lines.append(aligner._get_line(bead[0], aligner.src_sents))\n",
        "        tgt_lines.append(aligner._get_line(bead[1], aligner.tgt_sents))\n",
        "    return src_lines, tgt_lines\n",
        "\n",
        "def trim_phrases(phrases, illegal_phrases=None, threshold=0.4):\n",
        "    phrases_out = []\n",
        "    for phrase_idx, phrase in enumerate(phrases):\n",
        "        if phrase['align_score'] < threshold:\n",
        "            continue\n",
        "        if contains_illegal_phrases(phrase, illegal_phrases):\n",
        "            continue\n",
        "        if len(phrases_out) == 0:\n",
        "            phrases_out.append(phrase)\n",
        "        elif phrase['target_start'] >= phrases_out[-1]['target_end']:\n",
        "            phrases_out.append(phrase)\n",
        "    return phrases_out\n",
        "\n",
        "def get_alignments(src, target, phrases, pipe, context_sep=' \\u00b6 '):\n",
        "    remove_ids = []\n",
        "    for phrase_idx, phrase in enumerate(phrases):\n",
        "        alignment_data = target[:phrase['target_start']] + context_sep + target[phrase['target_start']:phrase['target_end']] + context_sep + target[phrase['target_end']:]\n",
        "        try:\n",
        "            result = pipe(alignment_data, src)\n",
        "            phrase['src_start'] = result['start']\n",
        "            phrase['src_end'] = result['end']\n",
        "            phrase['src_text'] = result['answer']\n",
        "            phrase['align_score'] = result['score']\n",
        "        except:\n",
        "            remove_ids.append(phrase_idx)\n",
        "\n",
        "    for i, remove_id in enumerate(remove_ids):\n",
        "          del[phrases[remove_id-i]]\n",
        "\n",
        "def replace_phrases(snt_src, phrases, snt_target, pipe_align, illegal_phrases):\n",
        "    phrases = sorted(phrases, key=(lambda phrase: (phrase['target_start'], -phrase['target_end'])))\n",
        "    get_alignments(snt_src, snt_target, phrases, pipe_align)\n",
        "    phrases = trim_phrases(phrases, illegal_phrases)\n",
        "    if len(phrases) == 0:\n",
        "        return copy.copy(snt_src)\n",
        "    snt_out = copy.copy(snt_src[:phrases[0]['src_start']])\n",
        "    for idx, phrase in enumerate(phrases):\n",
        "        if idx == len(phrases) - 1:\n",
        "            snt_out = snt_out + snt_target[phrase['target_start']:phrase['target_end']] + snt_src[phrase['src_end']:]\n",
        "            continue\n",
        "        snt_out = snt_out + snt_target[phrase['target_start']:phrase['target_end']] + snt_src[phrase['src_end']:phrases[idx+1]['src_start']]\n",
        "    return snt_out\n",
        "\n",
        "\n",
        "def convert(src_lines, tgt_lines, pipe_src, pipe_target, pipe_align, illegal_phrases, data_idx, context_idx):\n",
        "    p_out = []\n",
        "    for src_line, tgt_line in zip(src_lines, tgt_lines):\n",
        "        if len(pipe_src(src_line).sentences) == 0:\n",
        "            continue\n",
        "        if len(pipe_target(tgt_line).sentences) == 0:\n",
        "            p_out.append(src_line)\n",
        "            continue\n",
        "        snt_src = pipe_src(src_line).sentences[0]\n",
        "        snt_target = pipe_target(tgt_line).sentences[0]\n",
        "        phrases = []\n",
        "        for entity in snt_target.entities:\n",
        "            phrases.append({'target_start': entity.start_char, 'target_end':entity.end_char})\n",
        "        for word in snt_target.words:\n",
        "            if word.upos == 'ADJ':\n",
        "                phrases.append({'target_start': word.start_char, 'target_end':word.end_char})\n",
        "        snt_result = replace_phrases(snt_src.text, phrases, snt_target.text, pipe_align, illegal_phrases)\n",
        "        p_out.append(snt_result)\n",
        "    return ' '.join(p_out), True"
      ],
      "metadata": {
        "id": "izb1tZXvYa_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_sep=' \\u00b6 '\n",
        "lang = lang_codes[0]\n",
        "x_nlp = get_stanza_pipeline(lang, use_gpu=True)\n",
        "en_nlp = get_stanza_pipeline('en', use_gpu=True)\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "xciYSvhJdKJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'/content/xquad/xquad.{lang}.json', 'r') as f:\n",
        "    x_obj = json.load(f)\n",
        "with open(f'/content/xquad/xquad.en.json', 'r') as f:\n",
        "    en_obj = json.load(f)"
      ],
      "metadata": {
        "id": "1ATRhEczfLkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_forget_it = 0\n",
        "with tqdm(total=len(x_obj['data'])) as pbar1:\n",
        "    for data_idx, (x_data, en_data) in enumerate(zip(x_obj['data'], en_obj['data'])):\n",
        "        with tqdm(total=len(x_data['paragraphs']), leave=False) as pbar2:\n",
        "            for context_idx, (x_paragraph, en_paragraph) in enumerate(zip(x_data['paragraphs'], en_data['paragraphs'])):\n",
        "                x_context = x_paragraph['context'].replace('\\ufeff','')\n",
        "                en_context = en_paragraph['context'].replace('\\ufeff','')\n",
        "\n",
        "                illegal_phrases = []\n",
        "                for x_qas in x_paragraph['qas']:\n",
        "                    illegal_phrases.append(x_qas['answers'][0]['text'])\n",
        "\n",
        "                src_lines, tgt_lines = align_sentences(x_context,en_context, lang, 'en')\n",
        "\n",
        "                cs_context, changed = convert(src_lines, tgt_lines, x_nlp, en_nlp, pipe_align, illegal_phrases, data_idx, context_idx)\n",
        "\n",
        "                if changed:\n",
        "                    forget_it = False\n",
        "                    for x_qas, illegal_phrase in zip(x_paragraph['qas'], illegal_phrases):\n",
        "                        if cs_context.find(x_qas['answers'][0]['text']) == -1:\n",
        "                            count_forget_it += 1\n",
        "                            forget_it = True\n",
        "                    if forget_it:\n",
        "                        continue\n",
        "                    x_paragraph['context'] = cs_context\n",
        "                    for x_qas, illegal_phrase in zip(x_paragraph['qas'], illegal_phrases):\n",
        "                        x_start = cs_context.find(x_qas['answers'][0]['text'])\n",
        "                        x_qas['answers'][0]['answer_start'] = x_start\n",
        "                        if not x_qas['answers'][0]['text'] == cs_context[x_start:x_start + len(x_qas['answers'][0]['text'])]:\n",
        "                            print(f'data_id = {data_idx} -- context_id = {context_idx}')\n",
        "                            print(f\"{cs_context[x_start:x_start + len(x_qas['answers'][0]['text'])]}\")\n",
        "                            print(x_qas['answers'][0]['text'])\n",
        "\n",
        "                pbar2.update()\n",
        "        pbar1.update()"
      ],
      "metadata": {
        "id": "Jos-MQL1oDIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_context = x_obj['data'][31]['paragraphs'][4]['context'].replace('\\ufeff','')\n",
        "# en_context =  en_obj['data'][31]['paragraphs'][4]['context'].replace('\\ufeff','')\n",
        "# qas_s = en_obj['data'][31]['paragraphs'][4]['qas']\n",
        "\n",
        "# illegal_phrases = []\n",
        "# for x_qas in qas_s:\n",
        "#     illegal_phrases.append(x_qas['answers'][0]['text'])\n",
        "\n",
        "# src_lines, tgt_lines = align_sentences(x_context,en_context, lang, 'en')\n",
        "\n",
        "# cs_context, changed = convert(src_lines, tgt_lines, x_nlp, en_nlp, pipe_align, illegal_phrases, 31, 4)"
      ],
      "metadata": {
        "id": "S3VUjbyKtNcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'/content/xquad_cs_context_only/xquad.{lang}.json'\n",
        "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "with open(path, 'w+', encoding='utf-8') as f:\n",
        "    json.dump(x_obj, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "EGb5baGhfLpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $path /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "nZdKENpA_bRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import Arabic\n",
        "\n",
        "nlp = Arabic()\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "doc = nlp(\"This is a sentence. This is another sentence.\")\n",
        "assert len(list(doc.sents)) == 2"
      ],
      "metadata": {
        "id": "TqQUleydHLPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.sents"
      ],
      "metadata": {
        "id": "ObOtNXtBHa_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}